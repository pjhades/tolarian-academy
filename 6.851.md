# 6.851 Takeaways

- [Hashing](#hashing)
- [Static Trees](#static-trees)


# Hashing

A hash function maps `u` keys into a table with `m` slots.
The actually used slot is denoted by `n`.

A deterministic hash function may result in bad performance (collision, search time ...),
because it cannot provide any guarantee in an adversarial setting.
So we want to randomly pick a hash function from a family.

## Properties

These randomly picked hash functions may have the following properties,
which, in certain scenarios, can provide performance guarantees.
(See links to Wikipedia pages for example constructions.)

- Totally Random: each key is hashed to any slot uniformly randomly.
- [Universal](https://en.wikipedia.org/wiki/Universal_hashing):
  any two distinct keys collide with probability at most `1/m`.
- [k-wise Independent](https://en.wikipedia.org/wiki/K-independent_hashing):
  any k distinct keys are hashed to any k slots (not necessarily distinct) independently and equally likely, i.e.,
  the joint probability is bounded by `1/(m^k)`.

## Chaining and FKS Perfect Hashing

With a universal hash function, the expected chain length is `O(n/m)`
and the variance is constant.
which is the same order as the load factor. If we update table size by
making `m = O(n)` then the length will be constant.
(There is also a high probability bound which assumes a totally random
hash function thus not very useful in practice.)

FKS perfect hashing provides even stronger guarantees. This algorithm
builds a static table in `O(n)`, searches a key in `O(1)` worst-case
with `O(n)` space. The idea is basically using another hash function
to turn each chain of length `L` into a small collision-free hash table
with size `O(L^2)`.

FKS has the properties:
- If universality is held, the expected space consumption will be `O(n^2/m)`.
If we keep `m = O(n)` then it will use linear space. 
- The expected number of collisions in a small table is constant. By setting
this constant, we can achieve the desired probability guarantees of collision,
and try different hash functions from the function family when building
the table.

## Linear Probing

Results on this topic:

- With a totally random hash function, the cost will be
expected `O(1/(t^2))` per operation and `O((1+t)n)` space.
If we choose `t=1` then the cost will be constant while doubing
the space.
- Later people found that the constant expected time could
be achieved by only a family `O(log(n))` independent hash functions.
- More recent results showed that such constant expected time
could be achieved by only a 5-independence.

## Cuckoo Hashing

The idea is to use two tables and two hash functions, and do
each operation in a "back-and-forth" pattern: we alternatively
apply the two hash functions on a key and keep looking for new
slot to place it. It provides `O(1)` worst-case time for searches
and deletes, `O(1)` expected time for inserts.

With a family of totally random or `O(log(n))`-wise independent
hash functions, we can get `O(1/n)` build failure probability,
where the keys are unsustainable with the hash table setting and
we have to start over.


# Static Trees

## Conversion between RMQ and LCA

To convert RMQ to LCA, build Cartesian tree:

```
void cartesian(array[i..j]) {
    if (j - i == 1) { // base case
        return tree {
            node = array[i],
            left = nil,
            right = nil,
        };
    }

    k = index_of_minimum(array);
    return tree {
        node = array[k],
        left = cartesian(array[i..k-1]),
        right = cartesian(array[k+1..j]),
    };
}
```

Actually by inserting nodes one by one into the tree a Cartesian tree can be built in `O(n)`.

To convert LCA to RMQ, do in-order traversal of the tree and record node depth along the way.
Then LCA is reduced to the argmin version of RMQ: it finds the index of the minimum (common ancestor).

So an RMQ problem with an arbitrary universe can be reduced to LCA,
which can then be reduced back to RMQ but in a 0, 1, 2... numbered universe.

## Solving RMQ

The most practical thing is to do query in `O(1)` time and `O(nlog(n))` space,
by storing ranges of length in two's powers: `array[k..k+2^x]`, then the answer
is the minimum of two smaller ranges.

To remove the `log(n)` factor from the space complexity, an important thought
is **indirection**, where we divide the array into small pieces of size `(1/2)log(n)`,
and only put the minimum in each small piece in the big array.

Then we can find the answer:

- in the big array, or
- in the right part of the left-most small piece, or
- in the left part of the right-most small piece.

The first case can be done in `O(1)` time with the power-of-two ranges.
The last two cases only require very tiny amount of work:

1. We can only find the position of the minimum, and refer to that position
to get the minimum itself. So we shift all the numbers in a small piece
so that the first number in the small piece is 0.
2. Then we convert a small piece into `+1/-1` arrays. This can be done
in `O(m)` where `m=(1/2)log(n)` by doing an Euler tour: a DFS tour starting
from the root of the Cartesian tree.
3. With the help of the previous 2 steps, we can work on a small piece
of relative values, consisting of only `+1` and `-1`,
with respect to the first element which is 0.
4. Because the length of each small piece is fixed,
the number of such possible sequences of `+1` and `-1` is fixed,
which is bounded by `2^((1/2)log(n))` which is `sqrt(n)`, this is
smaller than the number of small pieces `n/((1/2)log(n))` so
we only store each "type" of small piece once.
5. Also because the distinct type of small pieces is so small
we can use brute force in such small pieces, where we build a
lookup table for each possible left or right part of a small piece.
The number of such parts is `O(m^2)`, just like the upper bound
of sub-intervals.
6. Above all we can fit such lookup tables in `O(n)` space.

## Level Ancestor

LA problems ask for the `k`-th ancestor above a given node `x`.
Maybe it's not that practical but there are some important
thoughts that need to be noted.

Brute force can solve this in `O(1)` time and `O(n^2)` space
by building a lookup table from any node to any level.

Now we improve this by using jump pointers: we store a pointer
from a node to it's ancestors `1`, `2`, `4`, ... `2^k` levels above.
This idea can help because we can jump faster.

We can improve this more by using ladder decomposition: finding the longest
root-to-leaf path, and find all other paths "growing" from this path.
For each path we construct an array of twice the size of the path.
In this way, we can find the ancestor within `O(1)` on a path
(we have overlaps between paths because the array size is twice of
the path), and move to another path like we do with jump pointers.

This ladder decomposition can be combined with jump pointers to
minimize the number of "jump"s in bad cases.

We can improve this by indirection. The detailed complexity analysis
is not very attractive. The important thing is, in a tree we can't simply
make small pieces like in the case of RMQ. Instead we separate small trees
with a maximum from the upper part of the tree. The number of such small
trees can be approximated by [Catalan numbers](https://en.wikipedia.org/wiki/Catalan_number#Applications_in_combinatorics).
